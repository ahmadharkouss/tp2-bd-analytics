{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 15:56:16 WARN Utils: Your hostname, ahmad-pc resolves to a loopback address: 127.0.1.1; using 192.168.65.9 instead (on interface enp0s1)\n",
      "24/03/28 15:56:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/28 15:56:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|       Day| AverageTemperature|\n",
      "+----------+-------------------+\n",
      "|2022-01-03|               0.49|\n",
      "|2022-01-04|-3.7500000000000013|\n",
      "|2022-01-05|  3.311363636363637|\n",
      "|2022-01-08|-3.8000000000000007|\n",
      "|2022-01-09| 1.4249999999999996|\n",
      "|2022-01-14| 4.1645161290322585|\n",
      "|2022-01-16| -7.924999999999999|\n",
      "|2022-01-18| 0.8083333333333335|\n",
      "|2022-01-22| -6.391666666666666|\n",
      "|2022-01-25| 2.9458333333333333|\n",
      "|2022-01-28|-0.6297872340425534|\n",
      "|2022-01-31| -3.895833333333332|\n",
      "|2022-02-01|-1.8833333333333322|\n",
      "|2022-02-02|  3.531111111111111|\n",
      "|2022-02-04|  4.135714285714287|\n",
      "|2022-02-07| 0.8871428571428575|\n",
      "|2022-02-08| 2.8500000000000014|\n",
      "|2022-02-09| 2.1333333333333333|\n",
      "|2022-02-11|  8.591666666666667|\n",
      "|2022-02-13| 0.3303030303030298|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, count, date_format\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"ex7-q1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Load the RDD\n",
    "rdd = spark.sparkContext.textFile(\"file:///home/ahmad/bd-analytics/data/725053-94728-2022\")\n",
    "\n",
    "# Filter and map to extract day and temperature\n",
    "temperature_rdd = rdd.filter(lambda line: len(line) >= 92) \\\n",
    "    .map(lambda line: (line[15:23], line[87], float(line[88:92]) / 10))   \\\n",
    "    .filter(lambda x: x[2] != 999.9) # ignore missing values usually reported as 9999 in ncdc noaa dataset\n",
    "\n",
    "#print(temperature_rdd.collect())\n",
    "\n",
    "# Adjust the temperature based on the sign\n",
    "adjusted_temperature_rdd = temperature_rdd.map(lambda x: (x[0], x[2] * (-1 if x[1] == '-' else 1)))\n",
    "\n",
    "# Reduce by key to calculate the sum and count of temperatures for each day\n",
    "# ex : (day1 , (temp1, 1)), (day1, (temp2, 1)) => (day1, (temp1 + temp2, 1 + 1))\n",
    "sum_count_rdd = adjusted_temperature_rdd.mapValues(lambda temp: (temp, 1)) \\\n",
    "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "# Map again to calculate the average temperature for each day\n",
    "average_temperature_rdd = sum_count_rdd.mapValues(lambda x: x[0] / x[1])\n",
    "\n",
    "#print(average_temperature_rdd.collect())\n",
    "\n",
    "# Convert RDD to DataFrame\n",
    "temperature_df = average_temperature_rdd.toDF([\"Day\", \"AverageTemperature\"])\n",
    "\n",
    "#convert the day column to date type manually YYYYMMDD to YYYY-MM-DD\n",
    "temperature_df = temperature_df.withColumn(\"Day\", to_date(col(\"Day\"), \"yyyyMMdd\"))\n",
    "# Show the DataFrame\n",
    "temperature_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:================================>                          (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|pickup_date|trip_count|\n",
      "+-----------+----------+\n",
      "| 2022-10-19|    120603|\n",
      "| 2022-10-11|    111627|\n",
      "| 2022-11-01|    111385|\n",
      "| 2022-10-06|    114403|\n",
      "| 2022-10-23|     97313|\n",
      "| 2022-10-18|    116275|\n",
      "| 2022-10-07|    115382|\n",
      "| 2022-10-20|    123044|\n",
      "| 2022-10-15|    117597|\n",
      "| 2022-10-24|    102383|\n",
      "| 2022-10-14|    117868|\n",
      "| 2022-10-05|    107012|\n",
      "| 2022-10-08|    113332|\n",
      "| 2022-10-26|    119781|\n",
      "| 2022-10-21|    119621|\n",
      "| 2022-10-10|     88671|\n",
      "| 2022-10-04|    101071|\n",
      "| 2022-10-17|    103878|\n",
      "| 2022-10-27|    122828|\n",
      "| 2022-10-30|     99003|\n",
      "+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the taxi data\n",
    "taxi_data = spark.read.option(\"header\", \"true\").option(\"inferschema\", \"true\").parquet(\"file:///home/ahmad/bd-analytics/data/all-2022-cleaned.parquet\")\n",
    "\n",
    "\n",
    "# Extract the date from the pickup datetime \n",
    "taxi_data = taxi_data.withColumn(\"pickup_date\", to_date(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "\n",
    "# Group by date and count the number of trips for each day\n",
    "daily_trip_count = taxi_data.groupBy(\"pickup_date\").agg(count(\"*\").alias(\"trip_count\"))\n",
    "\n",
    "# Show the daily trip count\n",
    "daily_trip_count.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------+\n",
      "|       Day|AverageTemperature|trip_count|\n",
      "+----------+------------------+----------+\n",
      "|2022-10-19| 8.779166666666663|    120603|\n",
      "|2022-10-11|15.504166666666668|    111627|\n",
      "|2022-11-01|16.265151515151512|    111385|\n",
      "|2022-10-06|15.816666666666666|    114403|\n",
      "|2022-10-23|14.072413793103447|     97313|\n",
      "|2022-10-18|11.589285714285714|    116275|\n",
      "|2022-10-07|           19.4875|    115382|\n",
      "|2022-10-20|10.250000000000002|    123044|\n",
      "|2022-10-15|14.937499999999998|    117597|\n",
      "|2022-10-24|13.958823529411768|    102383|\n",
      "|2022-10-14|13.920370370370366|    117868|\n",
      "|2022-10-05|13.293877551020405|    107012|\n",
      "|2022-10-08|12.937499999999998|    113332|\n",
      "|2022-10-26|18.860377358490567|    119781|\n",
      "|2022-10-21|12.124999999999998|    119621|\n",
      "|2022-10-10|           14.8125|     88671|\n",
      "|2022-10-04|  9.64901960784314|    101071|\n",
      "|2022-10-17|15.890000000000002|    103878|\n",
      "|2022-10-27|15.975000000000001|    122828|\n",
      "|2022-10-30|           11.4125|     99003|\n",
      "+----------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join the temperature data with the daily trip count\n",
    "joined_data = temperature_df.join(daily_trip_count, temperature_df.Day == daily_trip_count.pickup_date)\n",
    "#remove the pickup_date column\n",
    "joined_data = joined_data.drop(\"pickup_date\")\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import altair as alt\n",
    "\n",
    "joined_data = joined_data.withColumn(\"Day\", date_format(col(\"Day\"), \"yyyy-MM-dd\"))\n",
    "scatter_plot = alt.Chart(joined_data.toPandas()).mark_circle().encode(\n",
    "    x=alt.X('AverageTemperature', title='Average Temperature (Â°C)', scale=alt.Scale(zero=False)),\n",
    "    y=alt.Y('trip_count', title='Trip Count'),\n",
    "    tooltip=['Day', 'AverageTemperature', 'trip_count']\n",
    ").interactive().properties(\n",
    "    width=800,  # Set the width of the chart\n",
    "    height=600  # Set the height of the chart\n",
    ")\n",
    "\n",
    "# Save the plot to a file\n",
    "scatter_plot.save('scatter_plot.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
